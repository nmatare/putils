% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fquery.R
\name{fast_bq_query_with_gcs}
\alias{fast_bq_query_with_gcs}
\title{'fast_bq_query_with_gcs'}
\usage{
fast_bq_query_with_gcs(query, project_id, bucket, dataset, table, service_file,
  verbose = TRUE, download = TRUE, path = tempdir(), legacy_sql = TRUE,
  export_as = "csv.gz", ...)
}
\arguments{
\item{query}{A SQL query to be executed.}

\item{project_id}{Project name/id for the project which the client acts 
on behalf of}

\item{bucket}{A bucket found in the user's GCS specifying where the 
queried results shall be downloaded into}

\item{dataset}{A character vector of length one identifying 
the desired dataset}

\item{table}{A character vector of length one identifying 
the desired table in aforementioned dataset}

\item{service_file}{The path to a private key file (this file was given to 
you when you created the service account)
This file must be a JSON object including a private key 
and other credentials information (downloaded from 
the Google APIs console)}

\item{verbose}{(optional) Boolean control for progress display}

\item{download}{(optional) Boolean control whether compressed files 
should be automatically downloaded, or remain in GCS
Note: You must manually delete the temporary files 
from your GCS bucket if you don't desire to have 
this function explicitly download the files}

\item{path}{(optional) A character vector of length one specifying 
the desired temporary location to download queries 
results into. Defaults to base::tempdir()}

\item{legacy_sql}{(optional) Boolean control for legacy SQL syntax 
defaults to True}

\item{export_as}{(optional) The exported file format. Possible values 
include 'csv', 'csv.gz', or 'avro.' Defaults to 'csv.gz'}

\item{...}{(optional) Any pass-through arguments (**kwargs) to 
data.table::fread()}
}
\description{
Fast retrieval of user data from Google Big Query. The queried data are 
transferred to the user's specified Google Cloud Storage bucket
before the entire result is returned as a singular 'data.table' object.
}
\details{
For large amounts of data, this is the preferred method to retrieve queries. 
A user must specify a SQL query to Google Big Query. Big Query will run the 
query and return the results to a temporarily table created in the user's
specified dataset. The temporary table will then be transferred to Google 
Cloud Storage. The temporary files are then read directly into a data.table via
data.table::fread. All temporary files are then removed (both on the client
(locally) and on the server (cloud))

This function uses the python API to interact with Google Big Query and 
Google Cloud Storage. The library data.table::fread is used to read 'csv' or 
'csv.gz' files. Therefore, you must first install the 'data.table', 
'reticulate', and 'unixtools' R libraries, along with the below 
python google.cloud client libraries:

pip install --upgrade google-cloud-bigquery

pip install --upgrade google-cloud-storage
}
\examples{


}
\references{
https://cloud.google.com/bigquery/docs/cached-results

https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs

https://cloud.google.com/bigquery/docs/writing-results

https://google-cloud-python.readthedocs.io/en/latest/bigquery/generated/
google.cloud.bigquery.job.QueryJobConfig.html?highlight=QueryJobConfig

https://google-cloud-python.readthedocs.io/en/latest/bigquery
/usage.html#load-table-data-from-google-cloud-storage
}
\author{
Nathan Matare <email: nmatare@chicagobooth.com>
}
\keyword{Big}
\keyword{Cloud}
\keyword{Google}
\keyword{Query}
\keyword{Storage}
\keyword{fast}
